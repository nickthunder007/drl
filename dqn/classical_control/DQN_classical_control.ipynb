{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4Xo1Jez-QVdx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import random\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  def __init__(self, observations, actions, env, eps, disc, epochs, buffer_size, lr=0.01, batch_size=256):\n",
        "    super(Agent,self).__init__()\n",
        "    self.eps = eps\n",
        "    self.epochs = epochs\n",
        "    self.obs_shape = observations\n",
        "    self.num_actions = actions\n",
        "    self.env = env\n",
        "    self.action_buffer = deque(maxlen=buffer_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.lr = lr\n",
        "    self.disc = disc\n",
        "\n",
        "  def DQN(self):\n",
        "    model = torch.nn.Sequential(nn.Linear(*self.obs_shape, 128),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(128, self.num_actions),\n",
        "                              )\n",
        "    return model\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "\n",
        "    model = self.DQN()\n",
        "    model2 = copy.deepcopy(model)\n",
        "    lossFN = torch.nn.MSELoss()\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=self.lr)\n",
        "    losses = []\n",
        "    for i in range(self.epochs):\n",
        "      done = False;\n",
        "      score = 0\n",
        "      state = self.env.reset()\n",
        "      while not done:\n",
        "        state = torch.tensor(state,dtype=torch.float32)\n",
        "        qval = model(state)\n",
        "\n",
        "        if random.random() < self.eps:\n",
        "            action = torch.argmax(qval).item()\n",
        "        else:\n",
        "            action = np.random.randint(self.num_actions)\n",
        "\n",
        "        state_, reward, done, _ = self.env.step(action)\n",
        "\n",
        "        score += reward\n",
        "        state_ = torch.tensor(state_, dtype=torch.float32)\n",
        "        self.action_buffer.append((state, action, reward, state_, done))\n",
        "        state = state_\n",
        "\n",
        "        if len(self.action_buffer) >= self.batch_size:\n",
        "          minibatch = random.sample(self.action_buffer, self.batch_size)\n",
        "          state1_batch = [s for (s, a, r, s_, d) in minibatch]\n",
        "          action_batch = [a for (s, a, r, s_, d) in minibatch]\n",
        "          reward_batch = [r for (s, a, r, s_, d) in minibatch]\n",
        "          state2_batch = [s_ for (s, a, r, s_, d) in minibatch]\n",
        "          done_batch = [d for (s, a, r, s_, d) in minibatch]\n",
        "          q1 = model(torch.stack(state1_batch))\n",
        "          with torch.no_grad():\n",
        "              q2 = model2(torch.stack(state2_batch))\n",
        "\n",
        "          X = q1[torch.arange(0, self.batch_size), action_batch]\n",
        "          Y = torch.tensor(reward_batch) + self.disc * torch.max(q2, dim=1)[0] * (1-torch.tensor(done_batch,dtype=torch.int32))\n",
        "          Y = Y.detach()\n",
        "\n",
        "          loss = lossFN(Y, X)\n",
        "          optim.zero_grad()\n",
        "          losses.append(loss.item())\n",
        "          loss.backward()\n",
        "          optim.step()\n",
        "      scores.append(score)\n",
        "      model2.load_state_dict(model.state_dict())\n",
        "      if(self.eps < 0.975):\n",
        "        self.eps += 0.01\n",
        "      print(\"Epoch: \", i, \"score: \", score,\" eps: \",self.eps, \" action buffer:\", len(self.action_buffer))\n",
        "    env.close()\n",
        "    return scores,losses\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b37JL2jIjICI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\"\"\"available environments\n",
        "    MountainCar-v0\n",
        "    CartPole-v1\n",
        "    Acrobot-v1\"\"\"\n",
        "\n",
        "env = gym.make('MountainCar-v0')        #render_mode=\"human\"\n",
        "num_actions = env.action_space.n\n",
        "obs_shape = env.observation_space.shape\n",
        "epss = 0.0\n",
        "replay_sizze = 2048\n",
        "discc = 0.99\n",
        "epochs=500\n",
        "agent = Agent(obs_shape, num_actions, env, epss, discc, epochs, replay_sizze, lr=0.01, batch_size=252)\n",
        "scores,losses = agent.train()"
      ],
      "metadata": {
        "id": "XgmRFfOIoHFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(model, 'acrobot.pth')\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[1].plot(losses)\n",
        "axs[1].set_title(\"loss\")\n",
        "axs[0].plot(scores)\n",
        "axs[0].set_title(\"reward\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kMots-LkRXTl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}